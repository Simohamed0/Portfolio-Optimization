# conf/training/default.yaml

n_epochs: 100       # Number of training epochs
batch_size: 64      # Batch size for training
learning_rate: 3.0e-4 # Initial learning rate for the Adam optimizer
clip_grad_norm: 1.0   # Max norm for gradient clipping (set to null or 0 to disable)

loss:
  name: "negative_mean_return" # all options: "sharpe_ratio", ( this one not yet supported "mean_variance" ), 'negative_mean_return'